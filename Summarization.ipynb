{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cH_A6Ke1Y9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b0ab797-067c-47ff-ba12-63a7eede218f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fye-tRiOz0RA"
      },
      "source": [
        "#imports\n",
        "!pip install transformers\n",
        "!pip install sumy\n",
        "!pip install sentencepiece\n",
        "!pip install bert-extractive-summarizer \n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import re\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "import math\n",
        "from sumy.summarizers import AbstractSummarizer\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from absl import logging\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import re\n",
        "from summarizer import TransformerSummarizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dKL4x8t10Vf9",
        "outputId": "1f9ec46e-1d97-470b-fa30-e8f70af12d9b"
      },
      "source": [
        "data = pd.read_csv('/content/Tweets.csv')\n",
        "data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet:</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pi goes a long way at \\r\\n@NASA\\r\\n. It is use...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>While we're all in favor of a little extra pie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ready to take math to Mars and beyond on #PiD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Happy #PiDay! It‚Äôs March 14th, or 3.14 ‚Äì also ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>When we look at a slice of the night sky, ùûπ (o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Pi takes us far in space exploration. In fact,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>\"In May last year, I announced the creation of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>\"When we launched the Solidarity Response Fund...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>\"The money collected will be used to suppress ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>\"The plan calls for a total requirement of $1....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>\"But as you know, the #COVID19 pandemic is not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Why are you so dogematic, they ask</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Doge day afternoon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Origin of Doge Day Afternoon:\\r\\nThe ancient R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>BTC (Bitcoin) is an anagram of\\r\\nTBC(The Bori...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Both do mining &amp; use blocks &amp; chains</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>FSD Beta has now been expanded to ~2000 owners...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Doge spelled backwards is Egod</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Falcon 9‚Äôs first stage has landed on the Of Co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Falcon 9 launch of 60 Starlink satellites happ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Targeting Sunday, March 14 at 6:01 a.m. EDT fo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Falcon 9‚Äôs first stage has landed on the Just ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Falcon 9 launch of 60 Starlink satellites is t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Falcon 9‚Äôs first stage has landed on the Of Co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>The Biden-Harris Administration has been worki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>This afternoon, I‚Äôm joining Vice President Har...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>85% of American households will get direct che...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>All adult Americans will be eligible to get th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>From $1,400 checks and unemployment relief to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Big news: Congress just passed President Biden...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Hey, India. We‚Äôre rolling out our new fleet of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Proud of the program we have in India to hire ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Amazon partners with thousands of kirana store...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Thank you to our customers who donated relief ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Nice! Amazon ranked #1 best managed company. D...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Tweet:\n",
              "0   Pi goes a long way at \\r\\n@NASA\\r\\n. It is use...\n",
              "1   While we're all in favor of a little extra pie...\n",
              "2    Ready to take math to Mars and beyond on #PiD...\n",
              "3   Happy #PiDay! It‚Äôs March 14th, or 3.14 ‚Äì also ...\n",
              "4   When we look at a slice of the night sky, ùûπ (o...\n",
              "5   Pi takes us far in space exploration. In fact,...\n",
              "6   \"In May last year, I announced the creation of...\n",
              "7   \"When we launched the Solidarity Response Fund...\n",
              "8   \"The money collected will be used to suppress ...\n",
              "9   \"The plan calls for a total requirement of $1....\n",
              "10  \"But as you know, the #COVID19 pandemic is not...\n",
              "11                 Why are you so dogematic, they ask\n",
              "12                                 Doge day afternoon\n",
              "13  Origin of Doge Day Afternoon:\\r\\nThe ancient R...\n",
              "14  BTC (Bitcoin) is an anagram of\\r\\nTBC(The Bori...\n",
              "15               Both do mining & use blocks & chains\n",
              "16  FSD Beta has now been expanded to ~2000 owners...\n",
              "17                     Doge spelled backwards is Egod\n",
              "18  Falcon 9‚Äôs first stage has landed on the Of Co...\n",
              "19  Falcon 9 launch of 60 Starlink satellites happ...\n",
              "20  Targeting Sunday, March 14 at 6:01 a.m. EDT fo...\n",
              "21  Falcon 9‚Äôs first stage has landed on the Just ...\n",
              "22  Falcon 9 launch of 60 Starlink satellites is t...\n",
              "23  Falcon 9‚Äôs first stage has landed on the Of Co...\n",
              "24  The Biden-Harris Administration has been worki...\n",
              "25  This afternoon, I‚Äôm joining Vice President Har...\n",
              "26  85% of American households will get direct che...\n",
              "27  All adult Americans will be eligible to get th...\n",
              "28  From $1,400 checks and unemployment relief to ...\n",
              "29  Big news: Congress just passed President Biden...\n",
              "30  Hey, India. We‚Äôre rolling out our new fleet of...\n",
              "31  Proud of the program we have in India to hire ...\n",
              "32  Amazon partners with thousands of kirana store...\n",
              "33  Thank you to our customers who donated relief ...\n",
              "34  Nice! Amazon ranked #1 best managed company. D..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKEnGJwu3H2F",
        "outputId": "d6e1ad0f-f809-4387-a508-4025e74f3570"
      },
      "source": [
        ""
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweet:    Falcon 9‚Äôs first stage has landed on the Of Co...\n",
            "Name: 23, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52S4NyET1S0D"
      },
      "source": [
        "#data preparation\n",
        "data_nparray = np.array(data)\n",
        "data_array = []\n",
        "for tweet in data_nparray:\n",
        "  data_array.append(tweet[0])\n",
        "\n",
        "data_copy = data_array"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz_1RrEW1xPO"
      },
      "source": [
        "final_data = []\n",
        "for tweet in data_array:\n",
        "  tweet = tweet.replace(\"@\",\"\")\n",
        "  tweet = tweet.replace('\"',\"\")\n",
        "  tweet = tweet.replace(\"\\n\",\"\")\n",
        "  tweet = tweet.replace(\"#\",\"\")\n",
        "  tweet = tweet.replace(\":\",\"\")\n",
        "  tweet = tweet.replace(\"...\",\"\")\n",
        "  \n",
        "  final_data.append(tweet)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MTWRbDD3GzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d594e9-f29e-4a37-ca71-c95363bd228d"
      },
      "source": [
        "#embedding data\n",
        "emb_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
            "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
            "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder/4, Total size: 987.47MB\n",
            "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzRqvhC74ytN"
      },
      "source": [
        "embedded_final = emb_model(np.array(final_data))\n",
        "embedded_final\n",
        "\n",
        "pca = PCA(2)\n",
        "embedded_final_new = pca.fit_transform(embedded_final)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEbs_YP55lR0"
      },
      "source": [
        "#clustering\n",
        "cluster_method = KMeans(3, random_state=1)\n",
        "\n",
        "final_clusters = cluster_method.fit_predict(embedded_final_new)\n",
        "final_clusters_unique = np.unique(final_clusters)\n",
        "\n",
        "labels = cluster_method.labels_"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "cqUa2fSe6Lhq",
        "outputId": "1485ddc9-472c-479e-a81d-541163b11060"
      },
      "source": [
        "for i in final_clusters_unique:\n",
        "  plt.scatter(embedded_final_new[final_clusters == i , 0] , embedded_final_new[final_clusters == i , 1] , label = i)\n",
        "  \n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0i1wX-905UP"
      },
      "source": [
        "#preparation for TextRank Summarization\n",
        "final = []\n",
        "for tweet in final_data:\n",
        "  tweet = tweet.replace(\" \",\"\")\n",
        "  final.append(tweet)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l9i1L734lfi"
      },
      "source": [
        "class TextRankSummarizer(AbstractSummarizer):\n",
        "  epsilon = 1e-4\n",
        "  damping = 0.85\n",
        "  _delta = 1e-7 \n",
        "  _stop_words = frozenset()\n",
        "\n",
        "  @property\n",
        "  def stop_words(self):\n",
        "    return self._stop_words\n",
        "\n",
        "  @stop_words.setter\n",
        "  def stop_words(self, words):\n",
        "    self._stop_words = frozenset(map(self.normalize_word, words))\n",
        "\n",
        "  @stop_words.setter\n",
        "  def stop_words(self, words):\n",
        "    self._stop_words = frozenset(map(self.normalize_word, words))\n",
        "\n",
        "  def __call__(self, document, sentences_count):\n",
        "    self._ensure_dependencies_installed()\n",
        "    if not document.sentences:\n",
        "      return ()\n",
        " \n",
        "    ratings = self.rate_sentences(document)\n",
        "    return self._get_best_sentences(document.sentences, sentences_count, ratings)\n",
        "\n",
        "  @staticmethod\n",
        "  def _ensure_dependencies_installed():\n",
        "    if numpy is None:\n",
        "      raise ValueError(\"LexRank summarizer requires NumPy. Please, install it by command 'pip install numpy'.\")\n",
        "\n",
        "\n",
        "  def _create_matrix(self, document):\n",
        "    sentences_as_words = [self._to_words_set(sent) for sent in document.sentences]\n",
        "    sentences_count = len(sentences_as_words)\n",
        "    weights = numpy.zeros((sentences_count, sentences_count))\n",
        "\n",
        "    for i, words_i in enumerate(sentences_as_words):\n",
        "      for j, words_j in enumerate(sentences_as_words):\n",
        "        weights[i, j] = self._rate_sentences_edge(words_i, words_j)\n",
        "\n",
        "    weights /= (weights.sum(axis=1)[:, numpy.newaxis]+self._delta) \n",
        "    return numpy.full((sentences_count, sentences_count), (1.-self.damping) / sentences_count) + self.damping * weights\n",
        "\n",
        "  def _to_words_set(self, sentence):\n",
        "    words = map(self.normalize_word, sentence.words)\n",
        "    return [self.stem_word(w) for w in words if w not in self._stop_words]\n",
        "\n",
        "  def rate_sentences(self, document):\n",
        "    matrix = self._create_matrix(document)\n",
        "    ranks = self.power_method(matrix, self.epsilon) \n",
        "    return {sent: rank for sent, rank in zip(document.sentences, ranks)}\n",
        "\n",
        "  @staticmethod\n",
        "  def power_method(matrix, epsilon):\n",
        "    transposed_matrix = matrix.T\n",
        "    sentences_count = len(matrix)\n",
        "    p_vector = numpy.array([1.0 / sentences_count] * sentences_count)\n",
        "    lambda_val = 1.0\n",
        "\n",
        "    while lambda_val > epsilon:\n",
        "      next_p = numpy.dot(transposed_matrix, p_vector)\n",
        "      lambda_val = numpy.linalg.norm(numpy.subtract(next_p, p_vector))\n",
        "      p_vector = next_p\n",
        "\n",
        "    return p_vector\n",
        "\n",
        "  @staticmethod\n",
        "  def _rate_sentences_edge(words1, words2):\n",
        "    rank = 0\n",
        "    for w1 in words1:\n",
        "      for w2 in words2:\n",
        "        rank += int(w1 == w2)\n",
        "\n",
        "      if rank == 0:\n",
        "        return 0.0\n",
        "\n",
        "      assert len(words1) > 0 and len(words2) > 0\n",
        "      norm = math.log(len(words1)) + math.log(len(words2))\n",
        "      if numpy.isclose(norm, 0.):\n",
        "            # This should only happen when words1 and words2 only have a single word. Thus, rank can only be 0 or 1.\n",
        "        assert rank in (0, 1)\n",
        "        return rank * 1.0\n",
        "      else:\n",
        "        return rank / norm"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai269Wpw8a2G",
        "outputId": "7506f91e-3c72-4feb-eaa2-f520138136b0"
      },
      "source": [
        "#Ordinary rate sentences with TextRank\n",
        "model_sm = TextRankSummarizer()\n",
        "\n",
        "words1 = final_data[17].split(\" \")\n",
        "words2 = final_data[12].split(\" \")\n",
        "\n",
        "print(model_sm._rate_sentences_edge(words1, words2))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.36926937306885504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDq3s6rrjyna",
        "outputId": "5fcb6021-42ae-4db0-b24c-41fbb6c3dd89"
      },
      "source": [
        "#T5 summarization and rate sentences with TextRank\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "preprocessed_text1 = \"summarize: \"+final_data[17]\n",
        "preprocessed_text2 = \"summarize: \"+final_data[12]\n",
        "\n",
        "tokens_input1 = tokenizer.encode(preprocessed_text1, \n",
        "                                return_tensors=\"pt\", \n",
        "                                max_length=512, \n",
        "                                truncation=True)\n",
        "tokens_input2 = tokenizer.encode(preprocessed_text2, \n",
        "                                return_tensors=\"pt\", \n",
        "                                max_length=512, \n",
        "                                truncation=True)\n",
        "\n",
        "summary_ids1 = model.generate(tokens_input1,\n",
        "                             min_length=8, \n",
        "                             max_length=512, \n",
        "                             length_penalty=4.0)\n",
        "summary_ids2 = model.generate(tokens_input2,\n",
        "                             min_length=8, \n",
        "                             max_length=512, \n",
        "                             length_penalty=4.0)\n",
        "\n",
        "summary1 = tokenizer.decode(summary_ids1[0])\n",
        "summary2 = tokenizer.decode(summary_ids2[0])\n",
        "\n",
        "words1 = summary1.split(\" \")\n",
        "words2 = summary2.split(\" \")\n",
        "\n",
        "print(model_sm._rate_sentences_edge(words1, words2))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.12595227380626867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1byGepxHkjC7",
        "outputId": "b6324498-f5ca-44ea-ba24-15caf8eee4c6"
      },
      "source": [
        "#GTP2 summarization and rate sentences with TextRank\n",
        "model = TransformerSummarizer(transformer_type=\"GPT2\",\n",
        "                              transformer_model_key=\"gpt2-medium\")\n",
        "\n",
        "summary1 = ''.join(model(final_data[17], min_length=8, max_length=512))\n",
        "summary2 = ''.join(model(final_data[12], min_length=8, max_length=512))\n",
        "\n",
        "words1 = summary1.split(\" \")\n",
        "words2 = summary2.split(\" \")\n",
        "\n",
        "print(model_sm._rate_sentences_edge(words1, words2))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['h.2.attn.masked_bias', 'h.21.attn.masked_bias', 'h.6.attn.masked_bias', 'h.14.attn.masked_bias', 'h.20.attn.masked_bias', 'h.0.attn.masked_bias', 'h.19.attn.masked_bias', 'h.1.attn.masked_bias', 'h.11.attn.masked_bias', 'h.3.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.12.attn.masked_bias', 'h.23.attn.masked_bias', 'h.13.attn.masked_bias', 'h.5.attn.masked_bias', 'h.7.attn.masked_bias', 'h.22.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.15.attn.masked_bias', 'h.18.attn.masked_bias', 'h.4.attn.masked_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.36926937306885504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHiH-i9Hk4bP",
        "outputId": "b5f1eca4-a024-44ff-965a-b55c88ff673c"
      },
      "source": [
        "#XLNet summarization and rate sentences with TextRank\n",
        "model = TransformerSummarizer(transformer_type=\"XLNet\",\n",
        "                              transformer_model_key=\"xlnet-base-cased\")\n",
        "\n",
        "summary1 = ''.join(model(final_data[17], min_length=8, max_length=512))\n",
        "summary2 = ''.join(model(final_data[12], min_length=8, max_length=512))\n",
        "\n",
        "words1 = summary1.split(\" \")\n",
        "words2 = summary2.split(\" \")\n",
        "\n",
        "print(model_sm._rate_sentences_edge(words1, words2))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
            "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.36926937306885504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ-BdaGvgd3H"
      },
      "source": [
        "# roBERTa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCYErF45ggNW"
      },
      "source": [
        "from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaConfig\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
        "config.is_decoder = True\n",
        "\n",
        "model = RobertaForCausalLM.from_pretrained('roberta-base', config=config)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V62UZb3Xi-or"
      },
      "source": [
        "tokens1 = tokenizer.encode(\"summarize: \"+final_data[17], return_tensors=\"pt\" )\n",
        "tokens2 = tokenizer.encode(\"summarize: \"+final_data[12], return_tensors=\"pt\")\n",
        "\n",
        "summary1_ids = model.generate(tokens1, max_length=70, min_length=10, length_penalty=4.0, num_beams=2)\n",
        "summary2_ids = model.generate(tokens2, max_length=70, min_length=10, length_penalty=4.0, num_beams=2)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utzxTbrCrb4b",
        "outputId": "4b70c3fe-7ca0-41d2-ba65-f0c6f69225ee"
      },
      "source": [
        "summary1 = tokenizer.decode(summary1_ids[0])\n",
        "summary2 = tokenizer.decode(summary2_ids[0])\n",
        "\n",
        "words1 = summary1.split(\" \")\n",
        "words2 = summary2.split(\" \")\n",
        "\n",
        "print(model_sm._rate_sentences_edge(words1, words2))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.143476331982022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEG2bF9WvzLh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}